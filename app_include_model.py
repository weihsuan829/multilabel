import os
from flask import Flask, render_template, request
from model_mac import load_model, predict
import requests  # ç”¨æ–¼å‘¼å« Ollama API
import json

app = Flask(__name__)

# è¼‰å…¥æ¨¡å‹
model = load_model()

# LLM æ¨¡å‹åç¨±ï¼ˆå¯æ”¹ç‚º "qwen:7b", "mistral", "phi", ç­‰ï¼‰
LLM_MODEL = "gemma:2b"

# ä¿®å¾©å»ºè­°å­—å…¸
damage_tips = {
    "è®Šè‰²æ³›é»ƒ": "é¿å…é™½å…‰ç›´å°„ï¼Œä½¿ç”¨ç„¡é…¸ä¿å­˜ç´™æå„²å­˜ã€‚",
    "é»´æ–‘ï¼ˆé»ƒæ–‘ã€è¤æ–‘ï¼‰": "å¯ä½¿ç”¨é…’ç²¾æ£‰è¼•æ‹­æˆ–å†·å‡è™•ç†é˜²éœ‰ï¼Œç’°å¢ƒæ‡‰ä¿æŒé€šé¢¨ä¹¾ç‡¥ã€‚",
    "çšºè¤¶ç—•": "å¯ä½¿ç”¨å£“é‡æ¿æˆ–ä½æº«æ¿•å£“å±•å¹³ï¼Œé¿å…é«˜æº«ç†¨ç‡™é€ æˆæå‚·ã€‚",
    "ç´™å¼µè£‚ç—•": "å»ºè­°ä½¿ç”¨æ—¥æœ¬ç´™èˆ‡ä¸­æ€§è† èƒŒè²¼ä¿®å¾©ï¼Œé¿å…ç›´æ¥ä½¿ç”¨è† å¸¶ã€‚",
    "å­”æ´": "å¯ç”¨ä¿®è£œç´™å¼µå¡«è£œèƒŒè¥¯ï¼Œå¿…è¦æ™‚ä½¿ç”¨æ‰‹å·¥é»åˆå¼·åŒ–è„†å¼±å€åŸŸã€‚",
    "é‡‘å±¬é½ç—•": "ä½¿ç”¨é‚„åŸåŠ‘å»é™¤é½æ–‘ï¼Œä¸¦é¿å…æ¥è§¸å«éµé‡‘å±¬åŠé«˜æ¿•ç’°å¢ƒã€‚",
    "è† å¸¶è† ç—•": "æ‡‰ä»¥ä½æº«ç†±é¢¨æˆ–å°ˆæ¥­æº¶åŠ‘é™¤è† ï¼Œé¿å…ç”¨æ‰‹æ’•é™¤é€ æˆé¡å¤–ç ´å£ã€‚",
    "æ°´æ¼¬ç—•": "ä»¥é¢¨ä¹¾æ–¹å¼è™•ç†ä¸¦ç½®æ–¼ä½æ¿•ç’°å¢ƒä¸­ï¼Œé¿å…éœ‰èŒæ»‹ç”Ÿã€‚",
    "æ²¹å¢¨æ±¡æ¼¬": "é¿å…æ“¦æ‹­æ“´æ•£ï¼Œå¯è€ƒæ…®ä½¿ç”¨å›ºå®šåŠ‘è™•ç†å¢¨æ°´é‚Šç•Œã€‚",
    "é«’æ±¡": "ä½¿ç”¨åˆ·å­æˆ–æ©¡çš®æ“¦æ¸…æ½”ï¼Œé¿å…ä½¿ç”¨æ¿•å¸ƒå°è‡´æŸ“è‰²æˆ–æå‚·ã€‚"
}

import re

# æ ¼å¼è™•ç†å‡½å¼ï¼šå°‡ LLM è¼¸å‡ºæ–‡å­—è½‰ç‚ºçµæ§‹æ¸…æ¥šçš„å…§å®¹
def format_llm_output(text):
    import re

    # æ¸…é™¤ Markdown èˆ‡é›œé …ç¬¦è™Ÿ
    text = text.replace("**", "").replace("--", "-").strip()
    text = re.sub(r"æ–‡ä»¶å—æç‹€æ³åŠä¿å­˜å»ºè­°[:ï¼š]?", "", text, flags=re.IGNORECASE)

    # è¨­å®šè‡ªè¨‚æ¨™é¡Œé–‹é ­
    output = [
        
        "æª”æ¡ˆå—ææƒ…æ³ç¸½çµï¼šæœ¬æª”æ¡ˆåŒ…å«ä»¥ä¸‹æå®³ï¼Œå»ºè­°æ¡ç”¨ä¸‹åˆ—ä¿®å¾©æ–¹å¼ã€‚\n"
    ]

    # å®šç¾©å¯è­˜åˆ¥çš„æå£é¡å‹ â†’ è‡ªå‹•åŠ æ¢åˆ—åºè™Ÿ
    damage_keywords = ["çšºè¤¶ç—•", "é»´æ–‘", "é«’æ±¡", "å­”æ´", "è®Šè‰²æ³›é»ƒ", "æ²¹å¢¨æ±¡æ¼¬", "è† å¸¶è† ç—•", "æ°´æ¼¬ç—•", "é‡‘å±¬é½ç—•", "ç´™å¼µè£‚ç—•"]
    keyword_index = 1

    lines = text.splitlines()
    current_section = ""
    formatted_lines = []

    for line in lines:
        line = line.strip()

        if not line:
            continue  # è·³éç©ºè¡Œ

        # åµæ¸¬ä¿®å¾©é¡å‹é–‹é ­ï¼ˆä¾‹å¦‚ï¼šçšºè¤¶ç—•ï¼‰
        matched = next((kw for kw in damage_keywords if kw in line), None)
        if matched:
            # æ–°æ®µè½ï¼šåŠ ä¸Šæ¢åˆ—åºè™Ÿï¼‹ç²—é«”ï¼ˆå¾Œç«¯å‘ˆç¾ç‚ºç´”æ–‡å­—ï¼‰
            formatted_lines.append(f"{to_chinese_numeral(keyword_index)}ã€{matched}")
            keyword_index += 1
            continue

        # å…¶ä»–æ¢åˆ—å»ºè­°
        # ç¬¬ä¸€è¡Œç‚ºç¸½çµèªï¼Œä¸åŠ æ¢åˆ—ç¬¦è™Ÿ
        if not formatted_lines and not line.startswith("-"):
            formatted_lines.append(line)
            continue

        # å…¶ä»–è¡Œè£œä¸Šæ¢åˆ—ç¬¦è™Ÿ
        if not line.startswith("-"):
            line = "- " + line

        formatted_lines.append(line)

    # æ¯æ®µå¾Œè£œç©ºè¡Œ
    spaced_lines = []
    for line in formatted_lines:
        spaced_lines.append(line)
        if re.match(r"^(- )?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€", line):
            continue  # æ¨™é¡Œæœ¬èº«ä¸åŠ ç©ºè¡Œ
        if line.startswith("-"):
            continue
        spaced_lines.append("")

    output.extend(spaced_lines)
    return "\n".join(output).strip()


# è¼”åŠ©ï¼šå°‡ 1,2,3 è½‰ç‚º ä¸€ã€äºŒã€ä¸‰...
def to_chinese_numeral(n):
    numerals = "ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å"
    if 1 <= n <= 10:
        return numerals[n - 1]
    return str(n)




# æœ¬åœ° LLM æ•´åˆï¼šå‘¼å« Ollama æ¨¡å‹ä¸¦æ ¼å¼åŒ–çµæœ
def local_llm_summary(prediction_list, suggestion_list):
    if not prediction_list:
        return "æœªåµæ¸¬åˆ°æ˜é¡¯æå®³ï¼Œç„¡éœ€ä¿®å¾©å»ºè­°ã€‚"

    # ğŸ”§ çµ„ prompt çµ¦æ¨¡å‹
    prompt = "ä½ æ˜¯ä¸€ä½ç´™æœ¬ä¿®å¾©å°ˆå®¶ã€‚é€™å¼µæª”æ¡ˆçš„å—æç‹€æ³å¦‚ä¸‹ï¼š\n"
    for p, s in zip(prediction_list, suggestion_list):
        prompt += f"- {p}ï¼š{s}\n"
    prompt += "è«‹ç”¨ç¹é«”ä¸­æ–‡å¯«å‡ºå®Œæ•´ç¸½çµï¼ŒåŒ…æ‹¬æ¯é …æå®³èˆ‡ä¿å­˜å»ºè­°ã€‚"

    try:
        print("[DEBUG] ç™¼é€ prompt çµ¦ Ollama...")
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": "gemma:2b", "prompt": prompt},
            stream=True
        )

        # Step 1ï¼šä¸²æµæ¥æ”¶æ¨¡å‹å›æ‡‰
        result = ""
        for line in response.iter_lines():
            if line:
                data = json.loads(line.decode("utf-8"))
                result += data.get("response", "")

        # Step 2ï¼šæ ¼å¼åŒ–å›æ‡‰
        cleaned = result.strip()
        formatted = format_llm_output(cleaned)
        return formatted if formatted else "âš ï¸ æ¨¡å‹æ²’æœ‰ç”¢ç”Ÿå›æ‡‰ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

    except Exception as e:
        print("[ERROR] å‘¼å« LLM ç™¼ç”Ÿä¾‹å¤–ï¼š", e)
        return f"âš ï¸ å‘¼å«æœ¬åœ°æ¨¡å‹æ™‚å‡ºéŒ¯ï¼š{e}"

@app.route("/", methods=["GET", "POST"])
def index():
    prediction = []
    suggestions = []
    image_path = None
    summary = None  # ç¢ºä¿ summary åœ¨ GET ç‹€æ…‹ä¸‹ä¹Ÿæœ‰å®šç¾©

    if request.method == "POST":
        file = request.files.get("image")
        if file and file.filename != "":
            # å»ºç«‹å„²å­˜è³‡æ–™å¤¾
            upload_folder = "static/uploads"
            os.makedirs(upload_folder, exist_ok=True)

            # å„²å­˜åœ–ç‰‡
            image_path = os.path.join(upload_folder, file.filename)
            file.save(image_path)

            # æ¨¡å‹é æ¸¬
            prediction = predict(model, image_path)

            # å°æ‡‰å»ºè­°
            # suggestions = [damage_tips.get(p, "ç„¡å»ºè­°") for p in prediction]
            suggestions = [f"{p}ï¼š{damage_tips.get(p, 'ç„¡å»ºè­°')}" for p in prediction]
            # unique_predictions = list(dict.fromkeys(prediction))  # âœ… ä¿ç•™é †åºä¸”å»é‡è¤‡
            # suggestions = [damage_tips.get(p, "ç„¡å»ºè­°") for p in unique_predictions]

            # LLMç”¢ç”Ÿçš„æ•´é«”ä¿®å¾©èªªæ˜ï¼ˆé¡å¤–è£œå……ï¼Œä¸å–ä»£ suggestionsï¼‰
            summary = local_llm_summary(prediction, suggestions)

    return render_template("index.html",
                           image_path=image_path,
                           prediction=prediction,
                           suggestions=suggestions,
                           summary=summary)

if __name__ == "__main__":
    app.run(debug=True)
